date
ls  -ltr
avro
clear
ls -ltr
hive
ls
s -ltr
ls -ltr
hdfs dfs -ls
hdfs dfs -ls /
hdfs dfs -ls /user/
hdfs dfs -ls /user/shree624
hdfs dfs -ls /user/shree624/
hdfs dfs -ls /data/
ls -ltr
hdfs dfs -ls /
hdfs dfs -ls /databricks-datasets
hdfs dfs -ls /databricks-datasets/airlines
cd ..
l s-ltr
ls -ltr
ls -ltr | more
ls -ltr |wc -l
pwd
mkdir problems
cd problems
ls -ltr
vi Hive1.py
pyspark < Hive1.py 
cd problems/
pyspark < Hive1.py 
export SPARK_MAJOR_VERSION=2
pyspark < Hive1.py 
hdfs dfs -ls 
hdfs dfs -ls /
hdfs dfs -ls /public
hdfs dfs -ls /public/orders
hdfs dfs -ls /
hdfs dfs -ls /user
hdfs dfs -ls /user/shree624
hdfs dfs -mkdir /user/shree624
hdfs dfs -mkdir /user/shree624/Solutions
hdfs dfs -mkdir /user/shree624/Solutions/problem1
export SPARK_MAJOR_VERSION=2
pyspark < Hive1.py 
ls -ltr
hdfs dfs -cat /public/orders/part-00000 | head
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
clear
ls -ltr
hdfs dfs -cat /public/orders/part* | more
l s-ltr
ls -ltr
hdfs dfs -ls /user/shree624/Solutons/problem1/
hdfs dfs -ls /user/shree624/Solutions/problem1/
hdfs dfs -cat /user/shree624/Solutions/problem1/part*
hdfs dfs -cat /user/shree624/Solutions/problem1/part*.*
hdfs dfs -cat /user/shree624/Solutons/problem1/part-*.*
hdfs dfs -ls /user/shree624/Solutons/problem1/part-*.*
hdfs dfs -ls /user/shree624/Solutons/problem1/
hdfs dfs -ls /user/shree624/Solutions/problem1/
hdfs dfs -cat /user/shree624/Solutions/problem1/part*.*
pyspark < Hive1.py 
clear
cd
cd problems/
ls -ltr
pyspark < Hive1.py 
export SPARK_MAJOR_VERSION=2
pyspark < Hive1.py 
cd problems/
vi Hive1.py 
pyspark < Hive1.py 
export SPARK_MAJOR_VERSION=2
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
pyspark < Hive1.py 
vi Hive1.py 
clear
ls -ltr
hdfs dfs -ls /public/orders
hdfs dfs -head /public/orders/part-00000 
hdfs dfs -top /public/orders/part-00000 
hdfs dfs -cat /public/orders/part-00000 |more
ls -ltr
pyspark < hive2.py 
hdfs dfs -ls /users/shree624/Solutions/problem1
hdfs dfs -ls /users/shree624/Solutions
hdfs dfs -ls /user/shree624/Solutions
pyspark < hive2.py 
hdfs dfs -ls /user/shree624/Solutions/
pyspark < hive2.py 
hdfs dfs -ls /user/shree624/Solutions/problem1/
hdfs dfs -cat /user/shree624/Solutions/problem1/part*.*
hdfs dfs -cat /lhuser/shree624/Solutions/problem1/part*.*
hdfs dfs -cat /public/orders/part-00000
hdfs dfs -cat /user/shree624/Solutions/problem1/par*.*
hdfs dfs -cat /user/shree624/Solutions/problem1/par*.* | head
hdfs dfs -head /user/shree624/Solutions/problem1/par*.* 
hdfs dfs -more /user/shree624/Solutions/problem1/par*.* 
hdfs dfs -cat /user/shree624/Solutions/problem1/par*.* | more
 clear
ls -ltr
hdfs dfs copyToLocal /public/orders/par*.* .
hdfs dfs moveToLocal /public/orders/par*.* .
hdfs dfs -moveToLocal /public/orders/par*.* .
hdfs dfs -copyToLocal /public/orders/par*.* .
hdfs dfs -copyToLocal /public/orders/p*.* .
hdfs dfs -ls /public/orders/p*.* .
hdfs dfs -ls /public/orders/
hdfs dfs -copyToLocal /public/orders/part-00000 /usr/shree624/
pwd
hdfs dfs -copyToLocal /public/orders/part-00000 /home/shree624/
ls -ltr
vi part-00000 
awk -F',' '{print $3}' | sort | uniq
awk -F',' '{print $3}' part-00000 | sort | uniq
awk -F',' '{print $3}' part-00000 | sort | uniq | more
hdfs -dfs /user/shree624/Solutions/problem1/
hdfs dfs -ls /user/shree624/Solutions/problem1/
date
hdfs dfs -cat /user/shree624/Solutions/problem1/par*.*
pwd
cd problems/
vi hive2.py
avro-tools
scala
pwd
cd problems/
vi hive2.py 
history | grep =2
export SPARK_MAJOR_VERSION=2
cd problems/
pyspark < hive2.py 
pwd
cd problems/
ls -ltr
cd spark-warehouse/
ls -ltr
cd ..
ls -ltr
vi hive2.py 
ls -ltr
cd problems/
ls -ltr
history | grep SPARK
export SPARK_MAJOR_VERSION=2
pyspark < hive2.py 
history | grep SPARK
pyspark < hive2.py 
clear
hdfs dfs -ls /
hdfs dfs -ls /public/orders
hdfs dfs -cat  /public/orders/part-*
history SPARK
history | grep SPARK
export SPARK_MAJOR_VERSION=2
pyspark < hive3.py 
hdfs dfs -ls /user/shree624/Solutions/problem1/
date
pyspark < hive3.py 
hdfs dfs -ls /user/shree624/Solutions/problem3/
pyspark < hive3.py 
hdfs dfs -ls /user/shree624/Solutions/problem4/
pyspark < hive3.py 
hdfs dfs -ls /user/shree624/Solutions/problem4/
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
pyspark < hive3.py 
hdfs dfs -cat /public/orders/*.* | grep 'COMPLETE'
hdfs dfs -cat /public/orders/part* | grep 'COMPLETE'
hdfs dfs -cat /public/orders/part* | grep 'COMPLETE' | wc -l
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/part*
hdfs dfs -cat /user/shree624/Solutions/problem4/p*
pyspark < hive3.py 
hdfs dfs -cat /user/shree624/Solutions/problem4/p*
pyspark < hive3.py 
hive
clear
cd ..
ls -ltr
cd CREATE EXTERNAL TABLE emp.employee_external (
 id int,
 name string,
 age int,
 gender string)
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ','
 LOCATION '/user/hive/data/employee_externa
;
exit




;

cd nagasubin
pwd
ls -ltr
cd nagasuhin/
dde
cd akshayjdhotia/
ls -ltr
cd pythondemo/
l s-ltr
ls -ltr
cd retail/
ls -ltr
cd src
ls -ltr
cd main
ls -ltr
cd python/
ls -0ltr
ls -ltr
cat daily_revenue_per_product.py 
spark
pyspark
spark-shall
scala
spark-shell --packages org.apache.spark:spark-avro_2.11:2.3.0
history | grep SPARK
export SPARK_MAJOR_VERSION=2
spark-shell --packages org.apache.spark:spark-avro_2.11:2.3.0
spark-shell --packages org.apache.spark:spark-avro_2.12:2.4.0
spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.0
cd problems/
vi hive3.py
pyspark < hive2.py 
vi hive3.py 
cd problems/
vi hive3.py 
pyspark
pyspark2
clear
ls -ltr
vi hive2.py 
vi hive3.py 
pyspark2
cd problems/
pyspark2 < hive3.py 
pyspark2
cd problems/
ls -ltr
vi hive2.py
pyspark2 < hive2.py 
vi hive2.py
pyspark2 < hive2.py 
vi hive2.py
pyspark2 < hive2.py 
cd /user/shree624/
hdfs dfs -ls /user/shree624/
hdfs dfs -ls /user/shree624/.sparkStaging
hdfs dfs -ls /user/shree624/.sparkStaging/
hdfs dfs -ls /user/
hdfs dfs -ls /user/venkat21
hdfs dfs -ls /user/venkat21/prac1
hdfs dfs -ls/user/labsITVL90
hdfs dfs -ls /user/labsITVL90
hdfs dfs -ls /user/labsITVL90/
vi hive2.py 
pyspark2 < hive2.py 
vi hive2.py 
pyspark2 < hive2.py 
vi hive2.py 
pyspark2 < hive2.py 
vi hive2.py 
pyspark2 < hive2.py 
./bin/spark-submit --packages org.apache.spark:spark-avro_2.12:2.4.3
pyspark2
cd
ls .
ls -ltra
vi .bash_profile 
vi .bashrc
scala
spark-shell
cd /opt
ls -ltr
pyspark2
cd
cd problems/
vi hive2.py 
pyspark2 < hive2.py 
 cat hive2.py 
vi hive4.py 
pyspark2 < hive2.py 
pyspark2 < hive4.py 
pyspark2 < hive2.py 
/user/shree624/cca175_practice_test1/problem3/data/order_count_by_customer
pyspark2 < hive4.py 
cp hive4.py hive5.py
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
vi hive5.py 
pyspark2 <  hive5.py 
cat hive6.py 
vi hive6.py 
pyspark2 <  hive6.py 
cp hive6.py hive7.py
vi hive7.py 
pyspark5 < hive7.py 
pyspark2 < hive7.py 
vi hive7.py 
pyspark2 < hive7.py 
vi hive7.py 
pyspark2 < hive7.py 
cat hive7.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader
cat hive7.py 
vi hive7.py 
pyspark2 < hive7.py 
pyspark2 < hive6.py 
vi hive6.py 
pyspark2 < hive6.py 
cat hive6.py 
cat hive7.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader/
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader/part* | tail -1
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader/part* | head -1
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader/part* | hea
spark-shell
scala
cd .opt
ls -lt
cd /opt
l s-ltr
ls -ltr
cd  scala
ls =-ltr
ls -lr
cd ..
pwd
cd ..
ls -ltr
hdfs dfs -ls /
hdfs dfs -ls /public
hdfs dfs -ls /public/orders
pyspark2   --master yarn   --conf spark.ui.port=0   --conf spark.sql.warehouse.dir=/user/${USER}/warehouse
clear
ls -ltr
hdfs dfs -ls /public/
hdfs dfs -ls /public/retail_db
hdfs dfs -ls /public/retail_db/customers/
hdfs dfs -cat /public/retail_db/customers/par*
hdfs dfs -cat /public/retail_db/customers/par* | more
!
hdfs dfs -cat /public/retail_db/orders/par* | more
hdfs dfs -ls /public/h1b/h1b_kaggle/
hdfs dfs -cat /public/h1b/h1b_kaggle/h1b*
cd problems/
vi hive2.py 
pyspark2 < hive2.py 
vi hive2.py 
pyspark2 < hive2.py 
pyspark2 < hive2.py  | more
pyspark2 < hive2.py 
pyspark2 < hive2.py > output.txt
cat output.txt 
scala
spark-shell --packages com.databricks.spark.avro
spark-shell --packages com.databricks.spark.avro:2.11:2.4.0
spark-shell --packages com.databricks:spark-avro_2.11:2.4.0
spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.0
pyspark2 < hive2.py 
clear
spark2-shell
spark-shell --packages com.databricks.spark.avro:2.11:2.4.0
spark-shell --packages com.databricks.spark.avro:2.11:2.3.0
SET hive.metastore.warehouse.dir = /user/itversity/warehouse
CREATE DATABASE itversity_retail_db;
SET hive.metastore.warehouse.dir = /user/itversity/warehouse
cp hive2.py hive4.py
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
vi hive2.py 
hdfs dfs -copyToLocal /public/orders/part-00000 .
ls -ltr
pwd
vi hive4.py 
pyspark2 hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
vi hive4.py 
pyspark2 < hive4.py 
hdfs dfs -ls /user/shree624/
hdfs dfs -ls /user/shree624/cca175_practice_test1/
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem3
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem3/data
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem3/data/order_count_by_cusomter/
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem3/data/order_count_by_customer/
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem3/data/order_count_by_customer/part* | more
vi hive4.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem3/data/order_count_by_cusomter/
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem3/data/order_count_by_customer/part* | more
hdfs dfs -ls /public/
hdfs dfs -ls /public/retail_db_json/
hdfs dfs -ls /public/retail_db_json/orders/
hdfs dfs -ls /public/retail_db_json/orders_items/
hdfs dfs -ls /public/retail_db_json/order_items/
hdfs dfs -ls /public/retail_db_json/products/
hdfs dfs -cat /public/retail_db_json/products/part* | more
hdfs dfs -head /public/retail_db_json/products/part* | more
cp hive5.py hive6.py
vi hive6.py 
pyspark2 < hive6.py 
vi hive6.py 
pyspark2 < hive6.py 
ls -ltr
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment/par*
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment/par* | more
!
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment/par* | more
clear
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem7/data/h1b_data_noheader/part-00000-*
hdfs dfs -cat /user/shree624/cca175_practice_test1/problem6part-00000-*
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/
vi hive6.py 
pyspark2 <  hive6.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/
vi hive6.py 
pyspark2 <  hive6.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment
vi hive6.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment
vi hive6.py 
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/
hdfs dfs -ls /user/shree624/cca175_practice_test1/problem6/data/orders_pending_payment
vi hive6.py 
vi hive7.py 
pyspark2 < hive7.py 
hdfs dfs -ls /public/retail_db/
hdfs dfs -ls /public/retail_db/customers
hdfs dfs -ls /public/retail_db/customers/part-00000 | more
hdfs dfs -cat /public/retail_db/customers/part-00000 | more
vi hive9.py
pyspark2 < hive9.py
vi hive9.py
pyspark2 < hive9.py
vi hive9.py
pyspark2 < hive9.py
vi hive9.py
pyspark2 < hive9.py
vi hive9.py
clear
ls -ltr
vi hive9.py
hive
pwd
l s-ltr
ls -ltr
vi exam1.py
hdfs dfs -ls /user/shree624/problem2/
hdfs dfs -cat /user/shree624/problem2/part*
clear
vi exam1.py
vi exam1.py 
hive 
vi exam1.py 
ls -lr
hdfs dfs -ls /public
hdfs dfs -lsExamGroup.py
hdfs dfs -ls /public/nyse/
hdfs dfs -cat  /public/nyse/NYSE*.txt | more -n 3
hdfs dfs -cat  /public/nyse/NYSE*.txt | head -n 3
hdfs dfs -cat  /public/nyse/NYSE*.txt | head n -3
hdfs dfs -cat  /public/nyse/NYSE* | head n -4
hdfs dfs -cat  /public/nyse/NYSE* | head -n -4
pwd
ls
ipconfig
ip
ping gw03
ipconfig
whoami
clear
vi cca175_1.py
clear
vi cca175_2.py
spark-shell
spark2-shell --packages org.apache.spark.sql.avro
scala
vi cca175_1.py
ipconfig
ifconfig -a
ifconfig
git
clear
ls 0ltr
l s-ltr
ls -ltr
git sync
clear
ls -ltr
git@github.com:smolgar/CCA175.git
clone git@github.com:smolgar/CCA175.git
git clone git@github.com:smolgar/CCA175.git
echo "# CCA175" >> README.md
git init
git add README.md
git commit -m "first commit"
git config --global user.email "shree624@gmail.com"
git config --global user.name "Sreedhar"
git branch -M main
git config --global user.name "Sreedhar"
git commit -m "first commit"
git remote add origin https://github.com/smolgar/CCA175.git
git push -u origin main
git remote add origin https://github.com/smolgar/CCA175.git

git remote add origin https://github.com/smolgar/CCA175.git
git push -u origin main
clear
git push -u origin main
exit
